{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bfab6d-e17a-4b91-97c1-92052871e0d9",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Interactivity</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc724c4-f6a2-47f1-94c4-aee59ba5073a",
   "metadata": {},
   "source": [
    "<font color=\"blue\">All cells are alterable but some cells that are easy to work with interactively are highlighted during the notebook. These are highlighted with the phrase **Alterable Cell** above them in blue and have instructions.</font>\n",
    "\n",
    "<font color=\"blue\">There is also a **DIY Section** towards the end of the Notebook that will allow you to modify parameters and use the code for your own purposes.</font>\n",
    "\n",
    "<font color=\"blue\">**Running Cells**</font>\n",
    "\n",
    "- <font color=\"blue\">To increase performance on the interactive graphics, click **Runtime** in the menu at the top of the Notebook, select **Change runtime type** and then click **T4 GPU**.</font>\n",
    "- <font color=\"blue\">To run all the cells, click **Runtime** in the menu at the top of the Notebook and select **Run All**.</font>\n",
    "- <font color=\"blue\">Shortcut to run a single cell, click in the cell and type **Ctrl + Enter (PC)** or **Cmnd + Enter (Mac)**.\n",
    "- <font color=\"blue\">If Notebook is not running correctly, try running all the cells again, as cells being run in the wrong order can sometimes cause issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81726d5b-9720-4b46-a1a8-bf1bb1b4704d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10ae20-8087-4344-b415-301f20020988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33049285-b9ce-44c3-aeac-d845165fac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/parnell_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390592f-ac14-4913-adc3-a2febbfd4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ec71b-2a21-4593-82ed-d4de9ebef092",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5847d25d-5d9e-475c-b12a-9e12b719a12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf469ca-d46c-4c68-818a-55fec839173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hvplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d2b9e-1512-4d7f-8437-d52cac42013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66349daa-1cb0-4ad9-aa21-f472d0abf838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disable unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#enable interactive visualisations\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for working with files\n",
    "import os\n",
    "import glob\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from natsort import natsorted, os_sorted\n",
    "\n",
    "#libraries for data extraction and parsing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#libraries for data analysis and manipulation\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import collections\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "#NLP Libraries\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from bertopic import BERTopic\n",
    "\n",
    "#libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import holoviews as hv\n",
    "import hvplot.networkx as hvnx\n",
    "import plotly.express as px\n",
    "from shapely.geometry import box\n",
    "\n",
    "# Optimize notebook and Spacy settings\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set DPI for Matplotlib figures\n",
    "plt.rcParams['figure.dpi'] = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451a459-3c3c-4b25-aa5e-2e1dd2749189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _render(self, **kw):\n",
    "    \"\"\"\n",
    "    Rendering method for bokeh in Jupyter.\n",
    "    Returns the rendered output as a MIMEBundle.\n",
    "    \"\"\"\n",
    "    hv.extension('bokeh')\n",
    "    return hv.Store.render(self)\n",
    "hv.core.Dimensioned._repr_mimebundle_ = _render"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37418b14-1b42-423b-836c-75810e7dfa6a",
   "metadata": {},
   "source": [
    "### Data Extraction Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5311c8-db60-4078-8cf2-92dfeb312320",
   "metadata": {},
   "source": [
    "Functions which perform the various aspects of getting from a list of file paths through to extracting specific parts of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dad03f-af47-4004-97b7-e3f6fb5955a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_objects(file_paths):\n",
    "    '''\n",
    "    Takes either a list of file paths or a file path.\n",
    "    Returns a list of beautiful soup objects or single\n",
    "    beautiful soup object, depending on the input.\n",
    "    '''\n",
    "    if type(file_paths) == list:\n",
    "        soup_list = []\n",
    "        for path in file_paths:\n",
    "            with path.open(\"r\", encoding=\"utf-8\") as xml:\n",
    "                source = BeautifulSoup(xml, \"lxml-xml\")\n",
    "                soup_list.append(source)\n",
    "        return soup_list\n",
    "    else:\n",
    "        with file_paths.open(\"r\", encoding=\"utf-8\") as xml:\n",
    "            soup_object = BeautifulSoup(xml, \"lxml-xml\")\n",
    "        return soup_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78915af5-1dca-4e01-984a-89631458acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tei_extractor(soup_obj, element, attributes=False):\n",
    "    '''\n",
    "    Takes Beautiful soup object or list of objects,\n",
    "    element using element name and, where necessary, attributes.\n",
    "    Returns list of elements for all input files or a list of\n",
    "    elements for input file, depending on input.\n",
    "    '''\n",
    "    attrib_dict ={}\n",
    "    if attributes:\n",
    "        attrib_dict = {attr: True for attr in attributes}\n",
    "\n",
    "    if type(soup_obj) == list:\n",
    "        elem_ls = [obj.find(element, attrib_dict) for obj in soup_obj]\n",
    "        return elem_ls\n",
    "    else:\n",
    "        elem_ls = soup_obj.find_all(element, attrib_dict)\n",
    "        return elem_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116a258-4feb-44b5-a4a7-d9b76244f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tei_values(object_list, attribute=False):\n",
    "    '''\n",
    "    Takes a list of beautiful soup elements, if attribute\n",
    "    value is being extracted include name of that attribute.\n",
    "    Return element or attribute value depending on input(s)\n",
    "    '''\n",
    "    if attribute:\n",
    "        values = [obj[attribute] for obj in object_list]\n",
    "        return values\n",
    "    else:\n",
    "        values = [obj.get_text() for obj in object_list]\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567c5d0-0a3d-4285-8502-4c801a4b15f9",
   "metadata": {},
   "source": [
    "### Data Cleaning and Dataframe Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b3a849-1d2f-4eaf-81db-fabba7347860",
   "metadata": {},
   "source": [
    "Functions to perform text cleaning, remove stopwords, convert results into dataframe format and cleans dataframe format data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb07478-2f03-4e7d-9863-ab9b6071979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    '''\n",
    "    Takes as input a string, removes/replaces special characters, newlines,\n",
    "    possessive apostrophes, hyphens, underscores, digits and makes single space.\n",
    "    Keeps punctuation in place.\n",
    "    Returns clean string.\n",
    "    '''\n",
    "    text = text.replace(u\"\\xa0\", u\" \").replace(\"&\", \"and\").replace(\"|\", \" \")\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"â€™\", \"'\").replace(\"'s \", ' ')\n",
    "    text = text.replace(\"-\", \" \"). replace(\"â€“\", \" \").replace(\"_\", \" \").replace(\"â€”\", \" \")\n",
    "    non_digit_text = re.sub(r\"\\b\\d+\\b\", \"\", text)\n",
    "    sing_space_text = re.sub(r\"\\s\\s+\", \" \", non_digit_text)\n",
    "    sing_space_text = sing_space_text.strip()\n",
    "    return sing_space_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70ac37-a1ad-45c6-9a26-0719bf2fda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_removal(text):\n",
    "    '''\n",
    "    Takes as input a string and removes punctuation, removes extra spacing.\n",
    "    Returns string without punctuation.\n",
    "    '''\n",
    "    text = re.sub(r\"(?<!\\w)'|'(?!\\w)\", ' ', text)\n",
    "    no_punc_text = re.sub(r\"[^\\w\\s\\']\", ' ', text)\n",
    "    sing_space_text = re.sub(r'\\s+', ' ', no_punc_text).strip()\n",
    "    return sing_space_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da01f3-2b01-4bcd-92ce-45959ad224cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords):\n",
    "    ''' \n",
    "    Take as input a string and list of stopwords, tokenizes\n",
    "    string and removes words contained in stopwords.\n",
    "    Returns re-joined string without stopwords.\n",
    "    '''\n",
    "    tokenized_text = text.split()\n",
    "    non_stop_text = [token for token in tokenized_text if token not in stopwords]\n",
    "    return ' '.join(non_stop_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba707a-61ed-4fdf-8a01-6740b7e14aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(data, columns):\n",
    "    '''\n",
    "    Takes as input a list of lists of data and a list of columns.\n",
    "    Returns a dataframe.\n",
    "    '''\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.transpose()\n",
    "    df.columns = columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea351a5-117a-4c5b-87e2-51313aacebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_cleaning(dataframe, clean_column=None):\n",
    "    '''\n",
    "    Takes as input a dataframe and makes lowercase, strips leading and\n",
    "    trailing spaces, standardises apostrophes. Applies text_cleaning function\n",
    "    to column if identified as clean column parameter.\n",
    "    Returns lowercase/cleaned dataframe.\n",
    "    '''\n",
    "    lower_dataframe = dataframe.applymap(lambda x: x.lower())\n",
    "    lower_dataframe = lower_dataframe.applymap(lambda x: x.replace(\"â€™\", \"'\"))\n",
    "    if clean_column:\n",
    "        lower_dataframe[clean_column] = lower_dataframe[clean_column].apply(lambda x: text_cleaning(x))\n",
    "    clean_dataframe = lower_dataframe.applymap(lambda x: x.strip())\n",
    "    return clean_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20fb57-5b5b-4131-8346-5ef201e966e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_sentence_tokenize(dataframe, column, tokenizer):\n",
    "    \"\"\"\n",
    "    Takes as input a dataframe containing a column of strings, name of a column\n",
    "    to tokenize and a sentence tokenizer.\n",
    "    Applies tokenizer to the column, creating a list of sentences in each row,\n",
    "    uses explode to expand sentences, so each one has its own row.\n",
    "    Changes name of column to 'sentence' and applies punctuation removal function\n",
    "    now that column has been divided into sentences.\n",
    "    Returns dataframe where each row contains a single sentence with punctuation removed.\n",
    "    \"\"\"\n",
    "    dataframe[column] = dataframe[column].apply(lambda x: tokenizer.tokenize(x))\n",
    "    sents_df = dataframe.explode(column)\n",
    "    sents_df = sents_df.rename(columns={column:'sentence'})\n",
    "    sents_df['sentence'] = sents_df['sentence'].apply(lambda x: punct_removal(x))\n",
    "    return sents_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db43a7a-1591-44f7-8791-ac17b6cf62e4",
   "metadata": {},
   "source": [
    "### Data Filtering Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ae8c4-52f2-4ead-ba97-be9b578d73f4",
   "metadata": {},
   "source": [
    "Functions to filter dataframe using different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac94398-bad5-4850-8330-02b81e0e0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_cooccurrance_count(dataframe, column, word):\n",
    "    \"\"\"\n",
    "    Takes as input a dataframe containing a column of strings, name of column,\n",
    "    and word to exclude from counts.\n",
    "    Converts column contents into a list and uses Counter to find word\n",
    "    frequency for all words in list, excluding word to exclude from count.\n",
    "    Return sorted count list or empty list if input dataframe is empty.\n",
    "    \"\"\"\n",
    "    if dataframe.empty:\n",
    "        return []\n",
    "    else:\n",
    "        co_occur_words = ' '.join(dataframe[column]).split()\n",
    "        word_count = Counter(co_occur_words)\n",
    "        del word_count[word]\n",
    "        sort_count = sorted(word_count.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sort_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bac3e8-fa35-4dfc-9464-61c885718038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_keywords_by_year(dataframe, column, keywords, word_boundary):\n",
    "    \"\"\"\n",
    "    Takes as input a dataframe with date and text columns,\n",
    "    the name of the text column, a list of keywords or phrases.\n",
    "    If column value incorrect raise value error, if keywords not list or empty print error message.\n",
    "    If word boundary is set to True ensures only whole word matches are counted.\n",
    "    If word_boundary is set to False counts word as a substring as well.\n",
    "    Counts occurrences of each keyword in the text column, group these counts\n",
    "    by the year in which they occur.\n",
    "    \"\"\"\n",
    "    if column not in dataframe.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "    if not isinstance(keywords, list) or not keywords:\n",
    "        print(\"Keywords list is empty\")\n",
    "    \n",
    "    dataframe['date'] = pd.to_datetime(dataframe['date']) \n",
    "    dataframe['year'] = dataframe['date'].dt.year\n",
    "    df_year_groups = dataframe.groupby('year')\n",
    "    #apply dataframe_count_strings function to each group\n",
    "    #counting instances of each keyword within group\n",
    "    apply_func = lambda df: dataframe_count_strings(df, column, keywords, word_boundary)\n",
    "    keyword_counts_by_year = df_year_groups.apply(apply_func)    \n",
    "    keyword_counts_by_year = keyword_counts_by_year.reset_index(name='keyword_counts')\n",
    "    #expands count dictionaries for each year so they form columns in dataframe\n",
    "    keyword_counts_by_year = keyword_counts_by_year.join(\n",
    "        pd.json_normalize(keyword_counts_by_year[\"keyword_counts\"])\n",
    "    ).drop(columns=[\"keyword_counts\"])\n",
    "\n",
    "    return keyword_counts_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87502815-e7ff-4d1c-b9cf-7368a3bc138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_count_strings(dataframe, column, keywords, word_boundary):\n",
    "    \"\"\"\n",
    "    Takes as inputs dataframe, column name, keywords list and word boundary parameter.\n",
    "    If column value incorrect raise value error, if keywords not list or empty print error message.\n",
    "    Counts occurrences of specified keywords in the given column of dataFrame.\n",
    "    If word boundary is set to True ensures only whole word matches are counted.\n",
    "    If word_boundary is set to False counts word as a substring as well.\n",
    "    Returns a dictionary where the keys are the keywords and values their counts for the column.\n",
    "    \"\"\"\n",
    "    if column not in dataframe.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "    if not isinstance(keywords, list) or not keywords:\n",
    "        print(\"Keywords list is empty\")\n",
    "    \n",
    "    processed_keywords = {word: fr\"\\b{re.escape(word)}\\b\" if word_boundary else re.escape(word) for word in keywords}\n",
    "    word_counts = {}\n",
    "    for word, word_pattern in processed_keywords.items():\n",
    "        word_count = dataframe[column].str.findall(word_pattern, flags=re.IGNORECASE).str.len().sum()\n",
    "        word_counts[word] = word_count\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86081208-8cce-45a8-a2aa-4cb7ad966256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe_by_keywords(dataframe, column, keywords, word_boundary=True, match_all=True):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame based on keywords list, with options for matching whole words or substrings,\n",
    "    and for requiring all or any keywords to appear in each row.\n",
    "    Takes dataframe and column name, searches column for regex matches depending on parameters below.\n",
    "    - word_boundary: if True, only matches whole words. If False, matches substrings.\n",
    "    - match_all: if True, only rows where all keywords are present are returned. \n",
    "    If False, rows with any keyword are returned.\n",
    "    Returns filtered DataFrame.\n",
    "    \"\"\"\n",
    "    if not keywords:\n",
    "        return dataframe\n",
    "    processed_keywords = [fr\"\\b{re.escape(word)}\\b\" if word_boundary else re.escape(word) for word in keywords]   \n",
    "    pattern = \"|\".join(processed_keywords)\n",
    "    if match_all:\n",
    "        filtered_df = dataframe[dataframe[column].str.contains(pattern, case=False, regex=True)]\n",
    "        filtered_df = filtered_df[\n",
    "            filtered_df[column].apply(lambda text: all(re.search(word, text, flags=re.IGNORECASE) \n",
    "            for word in processed_keywords))\n",
    "        ]\n",
    "    else:\n",
    "        filtered_df = dataframe[dataframe[column].str.contains(pattern, case=False, regex=True, na=False)]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544f94dc-d20f-45fa-8feb-1fa103ca7b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe_by_keywords_exclude(dataframe, column, keywords, word_boundary=True, match_all=False):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame to exclude rows based on keywords list, \n",
    "    with options for matching whole words or substrings,\n",
    "    and for requiring all or any keywords to appear in each row match.\n",
    "    Takes dataframe and column name, searches column for regex matches depending on parameters below.\n",
    "    - word_boundary: if True, only matches whole words. If False, matches substrings.\n",
    "    - match_all: if True, only rows where all keywords are present are removed. \n",
    "    If False, rows with any keyword are removed.\n",
    "    Returns filtered DataFrame.\n",
    "    \"\"\"\n",
    "    if not keywords:\n",
    "        return dataframe\n",
    "    processed_keywords = [fr\"\\b{re.escape(word)}\\b\" if word_boundary else re.escape(word) for word in keywords]   \n",
    "    pattern = \"|\".join(processed_keywords)\n",
    "    if match_all:\n",
    "        filtered_df = filtered_df[\n",
    "            ~filtered_df[column].apply(lambda text: all(re.search(word, text, flags=re.IGNORECASE) \n",
    "            for word in processed_keywords))\n",
    "        ]\n",
    "    else:\n",
    "        filtered_df = dataframe[~dataframe[column].str.contains(pattern, case=False, regex=True, na=False)]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824a113-a9b8-4ede-8df5-d2914f9ff30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_date_window(dataframe, column, include_range=None, exclude_range=None):\n",
    "    \"\"\"\n",
    "    Takes as input a dataframe with a datetime column, column name, \n",
    "    a range for the date range to filter the dataframe by,\n",
    "    and a date range to exclude during the filtering process.\n",
    "    Creates masks for include_range and exclude_range if applicable,\n",
    "    and applies them as dataframe filters.\n",
    "    Returns filtered dataframe.\n",
    "    \"\"\"\n",
    "    mask = pd.Series(True, index=dataframe.index)\n",
    "    #range example: ['1880-01-01', '1885-01-01']\n",
    "    if include_range:\n",
    "        include_mask = (dataframe[column] >= include_range[0]) & (dataframe[column] <= include_range[1])\n",
    "        mask &= include_mask \n",
    "    #range example: ['1890-01-01', '1890-12-31']\n",
    "    if exclude_range:\n",
    "        exclude_mask = ~((dataframe[column] >= exclude_range[0]) & (dataframe[column] <= exclude_range[1]))\n",
    "        mask &= exclude_mask\n",
    "    return dataframe.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004729be-13d5-465e-beb9-b76ae8ac94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_column_filter(dataframe, column, include_values=None, exclude_values=None):\n",
    "    \"\"\"\n",
    "    Takes as input a dataframe with a string column, column name,\n",
    "    a list of values to include during filtering process,\n",
    "    and a list of values to exclude during filtering process.\n",
    "    Creates masks for include_values and exclude_values if applicable,\n",
    "    and applies them as dataframe filters.\n",
    "    Filters dataframe based on substring matching in the given column.\n",
    "    Returns filtered dataframe.\n",
    "    \"\"\"\n",
    "    mask = pd.Series(True, index=dataframe.index)\n",
    "    if include_values:\n",
    "        include_pattern = '|'.join(include_values)  \n",
    "        mask &= dataframe[column].str.contains(include_pattern, case=False, na=False)\n",
    "    if exclude_values:\n",
    "        exclude_pattern = '|'.join(exclude_values)\n",
    "        mask &= ~dataframe[column].str.contains(exclude_pattern, case=False, na=False)\n",
    "    return dataframe.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cd763-b339-4456-bee2-7252800a991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_apply_filters(dataframe, text_column, filters_dictionary, word_boundary, match_all,\n",
    "                            exclude_word_boundary, exclude_match_all, text_filtering=True):\n",
    "    \"\"\"\n",
    "    Apply multiple filters to a dataframe, taking as input a text column name, \n",
    "    filters dictionary, and text filtering parameters.\n",
    "    If relevant key is found in filters dictionary, apply dataframe filtering\n",
    "    with filtering function for the relevant category.\n",
    "    Filtering is both for the inclusion and exclusion of rows.\n",
    "    Return filtered dataframe.\n",
    "    \"\"\"\n",
    "    filtered_df = dataframe.copy()\n",
    "    if \"keywords\" in filters_dictionary and filters_dictionary[\"keywords\"] and text_filtering:\n",
    "        filtered_df = filter_dataframe_by_keywords(dataframe=filtered_df, column=text_column, \n",
    "                                                   keywords=filters_dictionary[\"keywords\"], \n",
    "                                                   word_boundary=word_boundary, match_all=match_all)\n",
    "        \n",
    "    if \"not_keywords\" in filters_dictionary and filters_dictionary[\"not_keywords\"] and text_filtering:\n",
    "        filtered_df = filter_dataframe_by_keywords_exclude(dataframe=filtered_df, column=text_column, \n",
    "                                                           keywords=filters_dictionary[\"not_keywords\"],  \n",
    "                                                           word_boundary=exclude_word_boundary, match_all=exclude_match_all)\n",
    "    \n",
    "    if filters_dictionary[\"places\"] or filters_dictionary[\"not_places\"]:\n",
    "        filtered_df = dataframe_column_filter(dataframe=filtered_df, column=\"place\", \n",
    "                                            include_values=filters_dictionary[\"places\"], \n",
    "                                            exclude_values=filters_dictionary[\"not_places\"])\n",
    "\n",
    "        \n",
    "    if filters_dictionary[\"publications\"] or filters_dictionary[\"not_publications\"]:\n",
    "        filtered_df = dataframe_column_filter(dataframe=filtered_df, column=\"publication\", \n",
    "                                              include_values=filters_dictionary[\"publications\"], \n",
    "                                              exclude_values=filters_dictionary[\"not_publications\"])\n",
    "        \n",
    "    if filters_dictionary[\"include_range\"] or filters_dictionary[\"exclude_range\"]:\n",
    "        filtered_df = dataframe_date_window(dataframe=filtered_df, column=\"date\", \n",
    "                                            include_range=filters_dictionary[\"include_range\"], \n",
    "                                            exclude_range=filters_dictionary[\"exclude_range\"])\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf45f51-700c-4627-8d2d-2400ef8fad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_counts(dataframe):\n",
    "    \"\"\"\n",
    "    Takes as input a dataframe, converts date column to year.\n",
    "    Counts the different years present in dataframe.\n",
    "    Returns a list where each tuple contains year, sentence count.\n",
    "    \"\"\"\n",
    "    dataframe[\"year\"] = dataframe[\"date\"].dt.year\n",
    "    year_counts = dataframe.groupby([\"year\"]).size()\n",
    "    return list(year_counts.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29942a9-2268-4f0b-ae35-cc1c512a2f91",
   "metadata": {},
   "source": [
    "### Visualisation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511bb1c-4493-4b6a-b142-9de6804b383e",
   "metadata": {},
   "source": [
    "Functions to produce visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42410db4-9088-4369-a5c2-9852bfc0a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_active_filters(filters_dict, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a summary of active filters and parameters for user output.\n",
    "    Takes two input dictionaries and removes items with empty values:\n",
    "    filters_dict: Dictionary of filtering criteria (e.g., places, publications).\n",
    "    kwargs: Additional keyword arguments for filtering parameters (e.g., booleans).\n",
    "    Returns a formatted string listing the active filters and parameters.\n",
    "    \"\"\"\n",
    "    #remove empty lists\n",
    "    active_filters = {key: value for key, value in filters_dict.items() if value}\n",
    "    #if parameter has falsey value, like empty list, string etc, make None\n",
    "    active_parameters = {key: (value if value else None) for key, value in kwargs.items()}\n",
    "\n",
    "    #print filters and parameters\n",
    "    output = f\"\\nActive Filters & Parameters:\\n\"   \n",
    "    if active_filters:\n",
    "        output += \"\\nFilters Applied:\"\n",
    "        for key, value in active_filters.items():\n",
    "            output += f\"  - {key.replace('_', ' ').title()}: {', '.join(map(str, value))}\\n\"\n",
    "    else:\n",
    "        output += \"\\nNo active filters applied.\\n\"\n",
    "    if active_parameters:\n",
    "        output += \"\\nParameters:\\n\"\n",
    "        for key, value in active_parameters.items():\n",
    "            output += f\"  - {key.replace('_', ' ').title()}: {value}\\n\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d258b097-4a8d-4579-8fdc-f3555c7ae926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bertopic_topics(dataframe, topic_model):\n",
    "    \"\"\"\n",
    "    Takes as input a dataframe and Bertopic topic model.\n",
    "    Extracts sentences for each dataframe row as lists.\n",
    "    Fits sentence list to topic model, creates dictionary of topics/sentences.\n",
    "    Returns topics, topic/sentence dictionary and topic model.\n",
    "    If there is an error due to too few topics, an error message is printed, \n",
    "    and the function does not return topics or topic_docs,\n",
    "    and function returns None value.\n",
    "    \"\"\"\n",
    "    sent_list = dataframe[\"sentence\"].to_list()\n",
    "    try:\n",
    "        topics, probs = topic_model.fit_transform(sent_list)\n",
    "        topic_docs = {topic: [] for topic in set(topics)}\n",
    "        for topic, doc in zip(topics, sent_list):\n",
    "            topic_docs[topic].append(doc)\n",
    "        return (topics, topic_docs, topic_model)\n",
    "    except Exception as e:\n",
    "        print(\"Error in topic model, could be too few topics to process. Try again with different filters and/or parameters.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ec79b-7673-4aa2-b1f1-99614e8b1905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bertopic_time(dataframe, topic_model):\n",
    "    \"\"\"\n",
    "    Takes as input a dataframe and Bertopic topic model.\n",
    "    Extracts dates and sentences for each dataframe row as lists.\n",
    "    Sends sentence list/dates to topics_over_time provided by BERTopic \n",
    "    to create visualisation.\n",
    "    Returns visualisation\n",
    "    \"\"\"\n",
    "    sent_list = dataframe[\"sentence\"].to_list()\n",
    "    date_list = dataframe[\"date\"].to_list()\n",
    "    #parameters can be adjusted for visualisation\n",
    "    topics_over_time = topic_model.topics_over_time(docs=sent_list,\n",
    "                                                timestamps=date_list,\n",
    "                                                nr_bins=30\n",
    "                                              )\n",
    "    fig = topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=15, height=500, width=1000)\n",
    "    fig.update_layout(yaxis_title = \"Count\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2176386-bc57-4d87-95fe-1e0b78de54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_visualisation(count_object, title):\n",
    "    \"\"\"\n",
    "    Takes as input a dictionary of items and their frequency scores\n",
    "    or a list of iterables which is converted into a frequency dictionary.\n",
    "    Returns a Matplotlib bar chart representing\n",
    "    value frequency, with title string as bar chart title\n",
    "    or a statement saying input dictionary is empty.\n",
    "    \"\"\"\n",
    "    if not count_object:\n",
    "        return \"Count object is empty, no visualisation returned.\"\n",
    "    if isinstance(count_object, list):\n",
    "        count_object = dict(count_object)\n",
    "    items, frequency = zip(*count_object.items())\n",
    "    x_pos = np.arange(len(items)) \n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    ax.bar(x_pos, frequency)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(items, rotation=90)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8a508-bf24-4b19-a7cf-decbca4e8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_graph(edges, edge_weights=False, node_size_mult=15):\n",
    "    \"\"\"\n",
    "    Takes list of lists of publication/location edges, returns a network graph \n",
    "    with node size changing depending on how many times node appears. Coloured \n",
    "    markers in graph depending on whether node contains comma (occuring in locations).\n",
    "    If edge_weights is True, then edge thickness changes increases in proportion to\n",
    "    the frequency of connections\n",
    "    \"\"\"\n",
    "    #initialise network graph object\n",
    "    G = nx.Graph()\n",
    "    #flatten edges list of lists to get nodes\n",
    "    nodes = [item for ls in edges for item in ls]\n",
    "    #initialise counter object\n",
    "    interactions = collections.Counter()\n",
    "    #count the appearances of nodes\n",
    "    for item in nodes:\n",
    "        interactions[item] += 1\n",
    "    #convert interactions to a dictionary\n",
    "    interactions = dict(interactions)\n",
    "    #create lists for node colours and sizes\n",
    "    col_map = []\n",
    "    size_list = []\n",
    "    #loop through each node and number of appearances\n",
    "    for key, value in interactions.items():\n",
    "        #add each node to network graph\n",
    "        G.add_node(key)\n",
    "        #add value to list created above, made larger using size multiplier parameter\n",
    "        size_list.append(value*node_size_mult)\n",
    "        #give places and publications different colours and append colour to list created above\n",
    "        if ',' in key:\n",
    "            col_map.append('red')\n",
    "        else:\n",
    "            col_map.append('blue')\n",
    "    #if edge_weights set to true, count each edge and convert to edge width\n",
    "    if edge_weights:\n",
    "        #use Counter to count the occurrences of each edge tuple\n",
    "        edge_tups = [tuple(item) for item in edges]\n",
    "        #count instances of each tuple\n",
    "        edge_counts = collections.Counter(edge_tups)\n",
    "        #convert the Counter object to a list of unique tuples with counts as a 'weight' dictionary at the end\n",
    "        edges = [(t[0], t[1], {'weight': count}) for t, count in edge_counts.items()]\n",
    "        #add edges to graph\n",
    "        G.add_edges_from(edges)\n",
    "        #get the edge weights as a dictionary\n",
    "        edge_counts = nx.get_edge_attributes(G, 'weight')\n",
    "        #create a list of edge widths based on the 'weight' attribute\n",
    "        edge_widths = [edge_counts.get((u, v), 1) for u, v in G.edges]\n",
    "        edge_widths = [value/2 for value in edge_widths]\n",
    "    #if not edge_weights, just add edge tuples to graph with one edge width\n",
    "    else:\n",
    "        #add edges to graph\n",
    "        G.add_edges_from(edges)\n",
    "        #set all edge widths to 0.5\n",
    "        edge_widths = 0.5\n",
    "    #establish graph style and node positions\n",
    "    pos = nx.spring_layout(G, k=0.1, iterations=20)\n",
    "    #return visualisation using networkx graph object, size_list, colour_map,\n",
    "    #labels, edge widths, width/height of image\n",
    "    return hvnx.draw(G, pos, edge_cmap='viridis', node_size=size_list, node_color=col_map, edge_width=edge_widths, \n",
    "              width=1000, height=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371d052-dc32-49be-950d-eb1075cfc28c",
   "metadata": {},
   "source": [
    "## Extracting Data from Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72604c9a-96fc-4705-af84-23f40b92c669",
   "metadata": {},
   "source": [
    "We begin by extracting the data we need from the speech report files and the speech register file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b00d88-1015-4461-a3e5-c37955135455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get paths for speech reports\n",
    "dir_path = Path('sources/')\n",
    "#get filepaths to all the XML files in the directory\n",
    "xml_files = (file for file in dir_path.iterdir() if file.is_file() and file.name.lower().endswith('.xml'))\n",
    "#sort xml file paths numerically using os_sorted library\n",
    "xml_files = os_sorted(xml_files)\n",
    "\n",
    "#get path for speech register file\n",
    "speech_file = Path('speeches/parnell_speeches.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5880ccb-715a-45ea-9c8a-c22d442a2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basename returns filename removing directory path.\n",
    "#split to remove \".xml\" extension so that we can use later as identifier\n",
    "filenames = []\n",
    "for path in xml_files:\n",
    "    filename = os.path.basename(path)\n",
    "    filename = filename.split(\".\")[0]\n",
    "    filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f428b2-3223-4d9b-b694-df1c48f9d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract speech reports and speech register as beautiful soup objects using function\n",
    "report_objects = soup_objects(xml_files)\n",
    "speech_object = soup_objects(speech_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16681b1c-925c-4d9a-9779-199e25f1f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#speech register beautiful soup objects extracted as lists using function\n",
    "#speech id, speech place and data\n",
    "speech_id_objs = tei_extractor(speech_object, element='speech_id')\n",
    "place_objs = tei_extractor(speech_object, element='place', attributes=['key'])\n",
    "date_objs = tei_extractor(speech_object, element='date', attributes=['when'])\n",
    "\n",
    "#speech reports beautiful soup objects extracted as lists using function\n",
    "#speech id, publication name, text\n",
    "speech_rep_objs = tei_extractor(report_objects, element='term', attributes=['key'])\n",
    "publication_objs = tei_extractor(report_objects, element='title', attributes=['key', 'level'])\n",
    "text_objs = tei_extractor(report_objects, element='body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c3fc6-26f8-4585-9703-6fc7fce96783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use function to extract values from tei elements extracted above\n",
    "speech_ids = tei_values(speech_id_objs)\n",
    "speech_places = tei_values(place_objs)\n",
    "#if speech place returns blank convert to \"unknown\"\n",
    "speech_places = [\"unknown\" if item == \"\" else item for item in speech_places]\n",
    "speech_dates = tei_values(date_objs, attribute='when')\n",
    "\n",
    "speech_rep_ids = tei_values(speech_rep_objs, attribute='key')\n",
    "publications = tei_values(publication_objs)\n",
    "#if publication returns blank convert to \"unknown\"\n",
    "publications = [\"unknown\" if item == \"\" else item for item in publications]\n",
    "texts = tei_values(text_objs)\n",
    "\n",
    "#for speech ids, if more than one id present, shown by inclusion of comma, convert into a list\n",
    "#some reports refer to more than one speech, so we need to capture them all \n",
    "speech_rep_ids = [item.split(',') if ',' in item else item for item in speech_rep_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4178a831-c39a-4dfe-ab5f-9d4c81ce381c",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aab953-ee14-4b56-8ca6-9ea349cdac07",
   "metadata": {},
   "source": [
    "The next stage is to convert the speech data into a dataframe format where we can easily manipulate it and get different subsets prior to analysis/visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00d2ff-f5fb-429d-8c76-408d827e6127",
   "metadata": {},
   "source": [
    "### Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc15b24-26ba-4ba1-8b9f-4641da4dcb1f",
   "metadata": {},
   "source": [
    "Below we create different types of dataframe that will be used for different forms of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd51bc-fca1-468d-b146-6f1ede9c83ee",
   "metadata": {},
   "source": [
    "### Main Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d32d43-742d-4d49-920c-e794a6269a32",
   "metadata": {},
   "source": [
    "Merges together speech report and speech register dataframes. The same report text will sometimes be repeated. This is the result of giving each speech id its own line in cases where a report covers more than one speech. This is necessary for forms of analysis where the speech id is important, but not for text-based analysis. Therefore, we create other dataframes below for text-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca454e2-9943-47cb-9f9c-b0a4bdc68cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for speech report dataframe, make list of lists of data and list of column names\n",
    "speech_rep_data = [filenames, speech_rep_ids, publications, texts]\n",
    "speech_rep_columns = ['filename', 'speech_id', 'publication', 'text']\n",
    "#use function to turn the above lists into dataframe\n",
    "speech_rep_df = create_dataframe(speech_rep_data, speech_rep_columns)\n",
    "#if there is more than one speech id for a report, the report data will appear as row for each id\n",
    "speech_rep_df = speech_rep_df.explode('speech_id')\n",
    "#use function to clean the text in the dataframe and standardise it\n",
    "speech_rep_df = dataframe_cleaning(speech_rep_df, clean_column='text')\n",
    "speech_rep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b096d-f5d9-4861-93ea-9124abd5d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for speech register dataframe, make list of lists of data and list of column names\n",
    "speech_data = [speech_ids, speech_places, speech_dates]\n",
    "speech_columns = ['speech_id', 'place', 'date']\n",
    "#use function to turn the above lists into dataframe\n",
    "speech_df = create_dataframe(speech_data, speech_columns)\n",
    "#use function to clean the text in the dataframe and standardise it\n",
    "speech_df = dataframe_cleaning(speech_df)\n",
    "#make speech id index so we can use it when we merge dataframes below\n",
    "speech_df.set_index('speech_id', inplace=True)\n",
    "#convert date column to datetime format, enables us to manipulate dataframe using dates\n",
    "speech_df['date'] = pd.to_datetime(speech_df['date'], format='%Y-%m-%d')\n",
    "#drop empty rows\n",
    "speech_df = speech_df.dropna(axis=0)\n",
    "speech_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75faa19c-d59e-4331-a447-34232ec85ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the speech register and speech report dataframes indexing on speech_id\n",
    "df_all = speech_rep_df.merge(speech_df, left_on='speech_id', right_index=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125e0bf-adc5-4a71-976e-93d3c88b3a2a",
   "metadata": {},
   "source": [
    "### Deduplicated Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be85d2-e4bc-48ef-9bd9-47a9c2b2759c",
   "metadata": {},
   "source": [
    "This dataframe removes duplicate rows for the same report. These are created in the main dataframe because sometimes a report will cover more than one speech, making it need more than one row. Removing rows means not all speeches are covered, but report text is not duplicated. This is needed for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbabe048-91cc-4f10-b24b-be8920530364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create deduplicated dataframe for text-based analysis\n",
    "#df_all will have multiple rows for same report if it covers multiple speeches\n",
    "#as each repeated report row will also have the accompanying text/metadata\n",
    "df_dedup = df_all.loc[~df_all.index.duplicated(keep='first')]\n",
    "df_dedup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072f857-acd6-475e-bd10-149a1612389b",
   "metadata": {},
   "source": [
    "### Sentence Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28111342-4ca7-499c-ac8d-807e3fadc01d",
   "metadata": {},
   "source": [
    "The main dataframe divided so that each sentence has a row with accompanying metadata. Again, the same report text will sometimes be repeated. Therefore, we create a deduplicated sentence dataframe below for text-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120e876-8641-4d15-9938-9d95b3d19d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize nltk abbreviation words, these will be added to the sentence tokenizer\n",
    "#they will prevent the tokenizer from reading some full stops as sentence-enders\n",
    "punkt_param = PunktParameters()\n",
    "#we can add our own abbreviation words, e.g. \"hon.\" and \"mr.\" frequently have full stops in the reports\n",
    "punkt_param.abbrev_types = set(['hon', 'mr', 'rev', 'dr', 'm.p', 'c.s', 'c.v', 'c.e', 't.l', 'j.r', 'j.j', 'a.j',\n",
    "                            'r.b', 'j.g', 'j.l', 'j.r', 'j.f', 'n.b', 'p.j', 'c.j', 't.d', 'r', 'p.p', 'l.p', 'c.c', 'wm',\n",
    "                            'capt', 'messrs', 'patk', '1d', '2d', '3d', '4d', '5d', '6d', '7d', '8d', '9d', '10d', '11d',\n",
    "                            '1/2d', '3/4d', 'prof', 'per cent', 'adm', '2s', '1,400,000/', '400,000/'])\n",
    "\n",
    "#initialize nltk sentence detector for dividing text into sentences\n",
    "sentence_tokenizer = PunktSentenceTokenizer(punkt_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7eeca-ad27-4e4c-a00f-93103b45287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy of the original dataframe\n",
    "speech_sents_df = df_all.copy()\n",
    "#tokenize so each row contains a speech report sentence, with accompanying metadata\n",
    "speech_sents_df = dataframe_sentence_tokenize(dataframe=speech_sents_df, column=\"text\", tokenizer=sentence_tokenizer)\n",
    "speech_sents_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7ee6f-ddbb-4872-8e5b-4bdbe1df1a9e",
   "metadata": {},
   "source": [
    "### Deduplicated Sentence Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4121fd18-c448-4035-9f0a-cdc9051db833",
   "metadata": {},
   "source": [
    "This dataframe removes duplicate sentence rows for the same report. Removing rows means not all speeches are covered, but report text is not duplicated. This is needed for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337969c-8e6b-42c8-a298-cab2ba400dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_sents_df_dedup = df_dedup.copy()\n",
    "speech_sents_df_dedup = dataframe_sentence_tokenize(dataframe=speech_sents_df_dedup, \n",
    "                                                    column=\"text\", tokenizer=sentence_tokenizer)\n",
    "speech_sents_df_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184fb7db-4fe8-4935-8d9d-5adf2672fabd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now also able remove punctuation from dataframe not divided into sentences\n",
    "df_all['text'] = df_all['text'].apply(lambda x: punct_removal(x))\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853cff25-28ae-4d46-a8f4-006eb7e906a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now also able remove punctuation from dataframe not divided into sentences\n",
    "df_dedup['text'] = df_dedup['text'].apply(lambda x: punct_removal(x))\n",
    "df_dedup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4de977-79cd-4a68-9538-7979a8277511",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">DIY Section</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe367d-3305-4e07-ba67-e5806de16a94",
   "metadata": {},
   "source": [
    "<font color=\"blue\">This section is to create your own parameters for analysis across all of the methodologies we have covered together. Change the parameters the cells below as per the instructions to get different results</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d928390e-10e1-4a68-a65c-c1191e6461fe",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Alterable Cells</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e1c5df-166f-4090-959d-8c8d1f648439",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Global Parameters</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376d30f-7f28-4215-a5e3-6943c6b83165",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Changing the contents of the cells below will set global parameters for Frequency, Network, and Topic Modelling sections. The frequency section also has its own alterable parameters, so please be aware that they will be affected by the global parameters.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536c2f8-33b5-4dd9-803f-7ba0b8d2106e",
   "metadata": {},
   "source": [
    "#### <font color=\"blue\">Run Code from this Cell</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066f32a-09ef-41d8-8387-962c862fa116",
   "metadata": {},
   "source": [
    "#### <font color=\"blue\">True/False Parameters</font>\n",
    "\n",
    "<ul style=\"color: blue;\">\n",
    "<li>word_boundary - whether or not to match substrings in text-based analysis, such as \"labo\" for \"labour\", \"labor\".</li>\n",
    "<li>match_all - whether to match all keywords in text-based analysis or match any from list.</li>\n",
    "<li>exclude_word_boundary - when excluding items in a text-based analysis, whether or not to match substrings.</li>\n",
    "<li>exclude_match_all - when excluding items in a text-based analysis, whether to match all keywords or any from list.</li>\n",
    "<li>stopwords_removal - whether to remove stopwords during some forms of text-based analysis.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ec130-4edf-4861-899a-a5ccbba62164",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_boundary = True\n",
    "match_all = True\n",
    "exclude_word_boundary = True\n",
    "exclude_match_all = False\n",
    "stopwords_removal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9f502-d238-4c5d-b58a-3d2f2e970389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stopwords list, we can then add our own stopwords to this list\n",
    "#stopwords are common words we can omit from our corpus if they are not useful for analysis\n",
    "stopwords_ls = stopwords.words('english')\n",
    "stopwords_ls.extend([\"every\", \"would\", \"cheers\", \"hisses\", \"applause\", \"could\", \"upon\", \"may\", \"go\",\n",
    "                   \"said\", \"say\", \"know\", \"far\", \"come\", \"put\", \"us\"])\n",
    "stopwords_ls = [item.lower() for item in stopwords_ls]\n",
    "stopword_ls = set(stopwords_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f785761-2fa1-4cfc-8e80-436024fbf3da",
   "metadata": {},
   "source": [
    "#### <font color=\"blue\">Global Filters</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac81028-7819-46e9-bf7f-a25646f326ad",
   "metadata": {},
   "source": [
    "<font color=\"blue\">These are filters that will be applied across all of the types of analysis below. Entering items into any of the fields in the global_filters section applies to the input dataset by default.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6ba2ac-dd0b-4ab7-9dcd-5acb75b5c53f",
   "metadata": {},
   "source": [
    "<ul style=\"color: blue;\">\n",
    "<li>keywords - text-based analysis, filter to get rows with keywords e.g. \"keywords\": [\"tenant\", \"farmer\"]</li>\n",
    "<li>not_keywords - text-based analysis, filter to remove rows with keywords e.g. \"not_keywords\": [\"tenant\", \"farmer\"]</li>\n",
    "<li>places - filter by whether substring in list matches location in row e.g. \"places\": [\"dublin\", \"london\"] would match rows where any of those items are contained in the \"place\" column.</li>\n",
    "<li>not_places - filter by whether substring in list matches location in row e.g. \"not_places\": [\"dublin\", \"london\"], but remove those rows instead.</li>\n",
    "<li>publications - filter by whether substring in list matches publication in row e.g. \"publications\": [\"freeman\", \"times\"] would match rows where any of those items are contained in the \"publication\" column.</li>\n",
    "<li>not_publications - filter by whether substring in list matches publication in row e.g. \"not_publications\": [\"freeman\", \"times\"], but remove those rows instead.</li>\n",
    "<li>include_range - filter by whether speech date falls into a range of dates e.g. \"include_range\": [\"1880-01-01\", \"1885-12-31\"]</li>\n",
    "<li>exclude_range - filter by whether speech date falls into a range of dates e.g. \"exclude_range\": [\"1884-01-01\", \"1884-12-31\"], but remove those rows instead.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf3fcf-0512-4481-a303-55b80a72e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_filters = {\n",
    "    \"keywords\": [],\n",
    "    \"not_keywords\": [],\n",
    "    \"places\": [],\n",
    "    \"not_places\": [],\n",
    "    \"publications\": [],\n",
    "    \"not_publications\": [],\n",
    "    \"include_range\": [],\n",
    "    \"exclude_range\": []\n",
    "}\n",
    "#make all values lowercase same as dataframes\n",
    "global_filters = {key: [item.lower() for item in value] for key, value in global_filters.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c3fbf3-c534-43ec-838b-930ed63627dd",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Frequency Parameters</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d07e022-9808-47dd-9b13-e4669b468c13",
   "metadata": {},
   "source": [
    "<font color=\"blue\">The cells below contain alterable parameters just for the frequency section, but bear in mind that they will be affected by non-text-based parameters or filters from the Global section above. For instance, if there is a global filter setting an inclusive date range of 1880 to 1885, the frequency parameters and section will only be applied to that date range.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80619a-f4e5-4607-a0c7-91022556bea7",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Amendable list of words to count for word frequency across dataset e.g. count_keywords = [\"parnell\", \"reform\"]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f800fc-b67d-4577-90d8-2dbe3ab0c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_keywords = [\"parnell\", \"reform\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c583c9d-550a-4eb4-b508-55cb1bdd1f8d",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Word to further analyse for the frequency of words in the same sentence as them. Also for visualising the count of a specific keyword by year.</font>\n",
    "\n",
    "<font color=\"blue\">Must be contained in the appropriate list above  e.g. context_visual_keyword = \"parnell\".</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e91cd34-c644-4e32-a451-09820db407ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_visual_keyword = \"parnell\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75d918-22d5-41b3-8c25-17dbf61d6ff0",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Words that will be used to extract the sentences in which they are all contained, can be used to see where the words appear together in their original sentence context e.g. context_keywords = [\"parnell\", \"question\"]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a454a-9438-4607-9106-d54b66f92247",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_keywords = [\"parnell\", \"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf36710-5ea9-4eaa-abca-be11d4ff139e",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Running the Section</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef52b0-04e5-48a2-b84a-34e6d12b54d3",
   "metadata": {},
   "source": [
    "<font color=\"blue\">To run this DIY section code **click in the cell with \"keywords\"** in it, then click **Runtime** in the menu at the top of the Jupyter Notebook, then click **Run cell and below**.</font>\n",
    "\n",
    "<font color=\"blue\">To see your results, scroll down and view the sections below.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86806b83-a76a-4f49-8078-f464c14fa1e6",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Currently Active Filters</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd1f745-4039-4c54-9014-fd92a83b6e55",
   "metadata": {},
   "source": [
    "<font color=\"blue\">This section shows the currently active parameters for the frequency section.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724e1b3-d1ff-40b2-adbb-99e18e2dce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_filters_parameters = display_active_filters(global_filters, word_boundary=word_boundary,\n",
    "    match_all=match_all, exclude_word_boundary=exclude_word_boundary, exclude_match_all=exclude_match_all,\n",
    "    stopwords_removal=stopwords_removal, count_keywords=count_keywords, context_visual_keyword=context_visual_keyword,\n",
    "    context_keywords=context_keywords)\n",
    "print(active_filters_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d91378-f190-4742-b030-845712265bd4",
   "metadata": {},
   "source": [
    "### Apply Filters to Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6030dbda-82b9-43cc-9d4f-a0a32c730b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_text_filt = dataframe_apply_filters(dataframe=df_dedup, text_column=\"text\", filters_dictionary=global_filters,\n",
    "                               word_boundary=word_boundary, match_all=match_all, \n",
    "                               exclude_word_boundary=exclude_word_boundary, exclude_match_all=exclude_match_all)\n",
    "df_all_non_text_filt = dataframe_apply_filters(dataframe=df_dedup, text_column=\"text\", filters_dictionary=global_filters,\n",
    "                               word_boundary=word_boundary, match_all=match_all, \n",
    "                               exclude_word_boundary=exclude_word_boundary, exclude_match_all=exclude_match_all,\n",
    "                               text_filtering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab3b7c7-322f-4478-8eaa-9a7aeda41a4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dedup_text_filt = dataframe_apply_filters(dataframe=df_dedup, text_column=\"text\", filters_dictionary=global_filters,\n",
    "                               word_boundary=word_boundary, match_all=match_all, \n",
    "                               exclude_word_boundary=exclude_word_boundary, exclude_match_all=exclude_match_all)\n",
    "df_dedup_non_text_filt = dataframe_apply_filters(dataframe=df_dedup, text_column=\"text\", filters_dictionary=global_filters,\n",
    "                               word_boundary=word_boundary, match_all=match_all, \n",
    "                               exclude_word_boundary=exclude_word_boundary, exclude_match_all=exclude_match_all,\n",
    "                               text_filtering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e2532-621b-457f-a6be-cee88ec84f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_df_text_filt =  dataframe_apply_filters(dataframe=speech_sents_df, text_column=\"sentence\", filters_dictionary=global_filters,\n",
    "                               word_boundary=word_boundary, match_all=match_all, \n",
    "                               exclude_word_boundary=exclude_word_boundary, exclude_match_all=exclude_match_all)\n",
    "sents_df_non_text =  dataframe_apply_filters(dataframe=speech_sents_df, text_column=\"sentence\", filters_dictionary=global_filters,\n",
    "                               word_boundary=word_boundary, match_all=match_all, \n",
    "                               exclude_word_boundary=exclude_word_boundary, exclude_match_all=exclude_match_all,\n",
    "                               text_filtering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a780e7-81af-47fa-bcc2-fc49150f72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_df_dedup_text_filt =  dataframe_apply_filters(dataframe=speech_sents_df_dedup, text_column=\"sentence\", filters_dictionary=global_filters,\n",
    "                               word_boundary=word_boundary, match_all=match_all, \n",
    "                               exclude_word_boundary=exclude_word_boundary, exclude_match_all=exclude_match_all)\n",
    "sents_df_dedup_non_text =  dataframe_apply_filters(dataframe=speech_sents_df_dedup, text_column=\"sentence\", filters_dictionary=global_filters,\n",
    "                               word_boundary=word_boundary, match_all=match_all, \n",
    "                               exclude_word_boundary=exclude_word_boundary, exclude_match_all=exclude_match_all,\n",
    "                               text_filtering=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c168f2-7652-4680-8827-08932d390740",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3debc7-62e7-4623-b635-adf3f37eaff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all user inputs lowercase\n",
    "count_keywords = [item.lower() for item in count_keywords]\n",
    "context_keywords = [item.lower() for item in context_keywords]\n",
    "context_visual_keyword = context_visual_keyword.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23272fb6-b189-46d6-8ffe-5a6b51f9f735",
   "metadata": {},
   "source": [
    "### Speech, Report and Sentence Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742bcd42-ed8c-44c1-9bea-aab836350831",
   "metadata": {},
   "source": [
    "#### Not affected by global filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab8770-320a-4aaa-a344-ee8f956797a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_year_counts = year_counts(speech_df)\n",
    "frequency_visualisation(count_object=speech_year_counts, title=\"Number of Speeches per Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4f278-4769-4cf5-b94e-6904b7cdb91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_year_counts = year_counts(df_dedup)\n",
    "frequency_visualisation(count_object=report_year_counts, title=\"Number of Speech Reports per Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc706fe-ca88-4423-8b6b-6cc6c3144e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_year_counts = year_counts(speech_sents_df_dedup)\n",
    "frequency_visualisation(count_object=sentence_year_counts, title=\"Number of Sentences per Year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d965b-7d1c-4b38-b14d-0e2f2f646fc1",
   "metadata": {},
   "source": [
    "### Keyword Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaaf1c7-5c48-4ec7-903d-c9ec2efe77e7",
   "metadata": {},
   "source": [
    "#### Not affected by global text filters, but affected by others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c11295-dff4-47c5-b563-47e4f4abae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = dataframe_count_strings(dataframe=df_dedup_non_text_filt, column=\"text\", \n",
    "                                   keywords=count_keywords, word_boundary=word_boundary)\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0986fa2-ff3c-490b-9d79-6995fcf05122",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_visualisation(count_object=df_count, title=\"Word Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb5678-ccdd-4139-a04e-2703959b0323",
   "metadata": {},
   "source": [
    "### Keyword Counts by Year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac92c4-6555-4f93-b23a-487a70ae5de6",
   "metadata": {},
   "source": [
    "#### Not affected by global text filters, but affected by others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606041ba-92d4-472c-9bda-1364db4058b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_word_counts_df = count_keywords_by_year(df_dedup_non_text_filt, column=\"text\", \n",
    "                                            keywords=count_keywords, word_boundary=word_boundary)\n",
    "year_word_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3ef83-2c68-4b22-ae0b-23914376b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_count_year = year_word_counts_df.groupby(\"year\")[context_visual_keyword].sum()\n",
    "keyword_count_year = list(keyword_count_year.items())\n",
    "frequency_visualisation(count_object=keyword_count_year, title=f'Frequency of \"{context_visual_keyword}\" per Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b27ea-3e76-472c-8f15-1525f6fedb16",
   "metadata": {},
   "source": [
    "### Sentence and Report Co-Frequency Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f488d3-4f70-42c8-862a-1db0e94d096b",
   "metadata": {},
   "source": [
    "#### Not affected by global text filters, but affected by others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577dded-ca8c-4aed-9752-59304080dbb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "co_freq_count = {}\n",
    "sent_co_freq_count = {}\n",
    "\n",
    "for count_word in count_keywords:\n",
    "    #filter the speech_sents_df_dedup dataframe for each word in count_keywords,\n",
    "    #so that rows where sentence contains word are returned\n",
    "    sent_word_df = filter_dataframe_by_keywords(dataframe=sents_df_dedup_non_text, column=\"sentence\", \n",
    "                                                    keywords=[count_word], word_boundary=word_boundary, \n",
    "                                                    match_all=True)\n",
    "    sent_word_count = dataframe_cooccurrance_count(dataframe=sent_word_df, column=\"sentence\", \n",
    "                                              word=count_word)\n",
    "    \n",
    "    word_df = filter_dataframe_by_keywords(dataframe=df_dedup_non_text_filt, column=\"text\", \n",
    "                                                    keywords=[count_word], word_boundary=word_boundary, \n",
    "                                                    match_all=True)\n",
    "    word_count = dataframe_cooccurrance_count(dataframe=word_df, column=\"text\", \n",
    "                                              word=count_word)\n",
    "    \n",
    "    \n",
    "    if stopwords_removal:\n",
    "        sent_word_count = [(k, v) for k, v in sent_word_count if k not in stopwords_ls]\n",
    "        word_count = [(k, v) for k, v in word_count if k not in stopwords_ls]\n",
    "    sent_co_freq_count[count_word] = sent_word_count[:15]\n",
    "    co_freq_count[count_word] = word_count[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f3146b-598c-4d43-b2a1-50532ed57a2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "co_freq_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6275b-53d4-4d1b-8848-41d07c0fa2c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if context_visual_keyword in co_freq_count:\n",
    "    report_visual_words = co_freq_count[context_visual_keyword]\n",
    "    frequency_visualisation(count_object=report_visual_words, title=f'Same Report as \"{context_visual_keyword}\"')\n",
    "else:\n",
    "    print(\"Context visual keyword is not in the count keywords list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323772b-9658-4721-9c35-56bb38d23cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_co_freq_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f63139-4d2d-4939-99e1-b0ecc1c4598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if context_visual_keyword in sent_co_freq_count:\n",
    "    sentence_visual_words = sent_co_freq_count[context_visual_keyword]\n",
    "    frequency_visualisation(count_object=sentence_visual_words, title=f'Same Sentence as \"{context_visual_keyword}\"')\n",
    "else:\n",
    "    print(\"Context visual keyword is not in the count keywords list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99006f-b516-4bb2-bf1c-c1f1fb232d9a",
   "metadata": {},
   "source": [
    "### Sentence Keywords Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9640a5-afc2-48a2-8574-5baf566e2b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if context_keywords:\n",
    "\n",
    "    sentence_keywords_df = filter_dataframe_by_keywords(dataframe=sents_df_non_text, column=\"sentence\", \n",
    "                                                        keywords=context_keywords, word_boundary=word_boundary, \n",
    "                                                        match_all=True)\n",
    "    if sentence_keywords_df.empty:\n",
    "        print(\"Sentence context search returned no results\")\n",
    "    \n",
    "    else:\n",
    "        speech_list = sentence_keywords_df['speech_id'].to_list()\n",
    "        report_list = sentence_keywords_df['filename'].to_list()\n",
    "        publication_list = sentence_keywords_df['publication'].to_list()\n",
    "        location_list = sentence_keywords_df['place'].to_list()\n",
    "        date_list = sentence_keywords_df['date'].to_list()\n",
    "        sentence_list = sentence_keywords_df['sentence'].to_list()\n",
    "        \n",
    "        for speech, place, report, pub, date, sent in zip(speech_list, location_list, \n",
    "                                                           report_list, publication_list, date_list, sentence_list):\n",
    "            print(report, pub, speech, place, date)\n",
    "            print(sent)\n",
    "            print('\\n')\n",
    "\n",
    "else:\n",
    "    print(\"Context keywords list is empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559b1f3-dcb6-4586-85b8-a3ac43045d9e",
   "metadata": {},
   "source": [
    "### Report Keywords Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ccb78-7195-43cf-8ea0-d79598e55c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if context_keywords:\n",
    "\n",
    "    report_keywords_df = filter_dataframe_by_keywords(dataframe=df_all_non_text_filt, column=\"text\", \n",
    "                                                        keywords=context_keywords, word_boundary=word_boundary, \n",
    "                                                        match_all=True)\n",
    "    if report_keywords_df.empty:\n",
    "        print(\"Report context search returned no results\")\n",
    "    \n",
    "    else:\n",
    "        speech_list = report_keywords_df['speech_id'].to_list()\n",
    "        report_list = report_keywords_df['filename'].to_list()\n",
    "        publication_list = report_keywords_df['publication'].to_list()\n",
    "        location_list = report_keywords_df['place'].to_list()\n",
    "        date_list = report_keywords_df['date'].to_list()\n",
    "        text_list = report_keywords_df['text'].to_list()\n",
    "        \n",
    "        for speech, place, report, pub, date, text in zip(speech_list, location_list, \n",
    "                                                           report_list, publication_list, date_list, text_list):\n",
    "            print(report, pub, speech, place, date)\n",
    "            print(text)\n",
    "            print('\\n')\n",
    "\n",
    "else:\n",
    "    print(\"Context keywords list is empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3db26-cf14-4e20-b94f-e31dc77333af",
   "metadata": {},
   "source": [
    "## Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a1c8b-9ff7-4dd9-8f7e-5eb7c07358e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = df_all_text_filt[['publication', 'place']].values.tolist()\n",
    "if not edges:\n",
    "    print(\"Network is empty, try again with different filters and/or parameters\")\n",
    "network_graph(edges, edge_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df18a3-7e21-46af-b98f-386c4b6d1839",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af82b98a-9e19-408f-a4c6-34f24f4d54d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize BERTopic topic model with parameters, uses a sentence transformers model to calculate topics\n",
    "topic_model = BERTopic(embedding_model=\"all-MiniLM-L6-v2\", nr_topics=\"auto\", top_n_words=15, min_topic_size=15)\n",
    "topic_error_msg = \"The topics list has too few topics to process. Try again with different filters and/or parameters.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23dd23-2314-4580-9846-ef10ecb8a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_df = sents_df_dedup_text_filt.copy()\n",
    "if stopwords_removal and not topic_model_df.empty:\n",
    "    topic_model_df['sentence'] = topic_model_df['sentence'].apply(lambda x: remove_stopwords(x, stopwords_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cfffee-9cb8-43be-837f-00cd793628a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = None\n",
    "if not topic_model_df.empty:\n",
    "    topics_data_all = bertopic_topics(topic_model_df, topic_model)\n",
    "    if topics_data_all:\n",
    "        model_data = topics_data_all[2]\n",
    "else:\n",
    "    print(topic_error_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c95a52-0139-440b-82e8-45504c929483",
   "metadata": {},
   "source": [
    "### Topics List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14461a10-98ca-4032-a45a-92831879337b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_data:\n",
    "    output = model_data.get_topics()\n",
    "else:\n",
    "    output = topic_error_msg\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df9179-564f-466e-b41f-f024909fcfa9",
   "metadata": {},
   "source": [
    "### Clustering Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa83499-1fb9-4870-86be-61e4ffec46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_data:\n",
    "    output = model_data.visualize_hierarchy()\n",
    "else:\n",
    "    output = topic_error_msg\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a990c2b6-f616-4e1f-83f6-fa02ede16f37",
   "metadata": {},
   "source": [
    "### Topics over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7f671-42a4-4279-ae96-37b3ee1209bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_data:\n",
    "    output = bertopic_time(topic_model_df, model_data)\n",
    "else:\n",
    "    output = topic_error_msg\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class env 2025",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
